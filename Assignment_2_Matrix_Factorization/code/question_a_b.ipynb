{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# Remove all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set env CUDA_LAUNCH_BLOCKING=1\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Retina display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "try:\n",
    "    from einops import rearrange\n",
    "except ImportError:\n",
    "    %pip install einops\n",
    "    from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the image\n",
    "if os.path.exists('dog.jpg'):\n",
    "    print('dog.jpg exists')\n",
    "else:\n",
    "    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O dog.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.io.read_image(\"dog.jpg\")\n",
    "print(img.shape)\n",
    "plt.imshow(rearrange(img, 'c h w -> h w c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img[:, X_mid-5*N_size:X_mid+5*N_size, Y_mid-5*N_size:Y_mid+5*N_size] // \n",
    "# X_mid, Y_mid = 700, 850 and N_size = 30\n",
    "\n",
    "X_mid = 700\n",
    "Y_mid = 875\n",
    "crop = img[:, X_mid-5*40:X_mid+5*40, Y_mid-5*40:Y_mid+5*40]\n",
    "plt.imshow(rearrange(crop, 'c h w -> h w c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a rectangular block of 30X30 (patch sie = 30) is assumed missing from the image (patch center is (X_mid, Y_mid)).\n",
    "def mask_image_structured(img, location, patch_size):\n",
    "    masked_img = img.clone().float()\n",
    "    mask = torch.zeros_like(img, dtype=torch.bool)\n",
    "    x_start = location[0] - int(patch_size/2)\n",
    "    y_start = location[1] - int(patch_size/2)\n",
    "    x_end = x_start + patch_size\n",
    "    y_end = y_start + patch_size\n",
    "    mask[:, x_start:x_end, y_start:y_end] = True\n",
    "    masked_img[mask] = float('nan')  # Note: This will only work if img is of floating-point dtype\n",
    "    return masked_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a random subset of 900 (30X30) pixels is missing from the image.\n",
    "\n",
    "def mask_image_unstructured(image_tensor, patch_size):\n",
    "    masked_image_tensor = image_tensor.clone().float()\n",
    "    _, height, width = masked_image_tensor.shape\n",
    "\n",
    "    # Generate all possible indices and shuffle them. Take the first 900 indices\n",
    "    indices = [(x, y) for x in range(height) for y in range(width)]\n",
    "    random.shuffle(indices)\n",
    "    selected_indices = indices[:patch_size*patch_size]\n",
    "    random_x = [x for x, _ in selected_indices]\n",
    "    random_y = [y for _, y in selected_indices]\n",
    "    masked_image_tensor[:, random_x, random_y] = float('nan')\n",
    "    return masked_image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the loss function to ignore NaN values\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def factorize(A, k, device=torch.device(\"cpu\")):\n",
    "    \"\"\"Factorize the matrix D into A and B\"\"\"\n",
    "    A = A.to(device)\n",
    "    # Randomly initialize A and B\n",
    "    \n",
    "    W = torch.randn(A.shape[0], k, requires_grad=True, device=device)\n",
    "    H = torch.randn(k, A.shape[1], requires_grad=True, device=device)\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam([W, H], lr=0.01)\n",
    "    mask = ~torch.isnan(A)\n",
    "    \n",
    "    # Train the model\n",
    "    for i in range(1000):\n",
    "        # Compute the loss\n",
    "        diff_matrix = torch.mm(W, H) - A\n",
    "        diff_vector = diff_matrix[mask]\n",
    "        loss = torch.norm(diff_vector)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    return W, H, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(reconstructed_img3D, original_img):\n",
    "    RMSE = torch.sqrt(((reconstructed_img3D - original_img) ** 2).mean()).item()\n",
    "    if RMSE == 0:\n",
    "        return 0, np.inf\n",
    "    PSNR = 10 * (math.log10((255)*2)/((RMSE)*2))\n",
    "    return RMSE, PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaping(reconstructed_img3D):\n",
    "    reconstructed_img_3D_reordered = reconstructed_img3D.permute(1, 2, 0)\n",
    "    reconstructed_img_np = reconstructed_img_3D_reordered.detach().cpu().numpy()\n",
    "\n",
    "    # to get all the values in the range [0, 255]\n",
    "    reconstructed_img_scaled = reconstructed_img_np - reconstructed_img_np.min()\n",
    "    reconstructed_img_scaled /= reconstructed_img_scaled.max()\n",
    "    reconstructed_img_scaled *= 255\n",
    "\n",
    "    reconstructed_img_scaled = np.uint8(reconstructed_img_scaled)\n",
    "    return torch.tensor(reconstructed_img_scaled).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructed_img(reconstructed_img, original_img, RMSE, PSNR, title, N_size):    \n",
    "    fig = plt.figure(figsize=(5, 2.5))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "\n",
    "    ax0.imshow(rearrange(reconstructed_img, 'c h w -> h w c'))\n",
    "    # ax0.set_title(f\"Reconstructed Image [Removed patch size = {N_size}*{N_size}]\")\n",
    "    ax0.set_title(f\"Reconstructed Image\")\n",
    "    ax1.imshow(rearrange(original_img, 'c h w -> h w c'))\n",
    "    ax1.set_title(f\"Original Image\")\n",
    "    for a in [ax0, ax1]:\n",
    "        a.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title, weight='bold')\n",
    "    fig.subplots_adjust(top=1.0)  # Adjust the vertical position of the super title\n",
    "    fig.text(0.5, 0.01, f\"RMSE = {RMSE:.2f}, PSNR = {PSNR:.2f}\", ha='center', weight='bold')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_img(img, k, N_size, X_mid, Y_mid, title, rect_patch):\n",
    "\n",
    "    crop = img[:, X_mid-5*40:X_mid+5*40, Y_mid-5*40:Y_mid+5*40]\n",
    "    \n",
    "    if rect_patch:  # a.structured\n",
    "        masked_img = mask_image_structured(crop, (5*40, 5*40), N_size)\n",
    "    else:           # b.unstructured\n",
    "        masked_img = mask_image_unstructured(crop, N_size)\n",
    "\n",
    "    # convert masked image into a 2D tensor to factorise\n",
    "    masked_img_2D = masked_img.reshape(-1, masked_img.size(-1))\n",
    "    W, H, loss = factorize(masked_img_2D, k, device=device)\n",
    "    reconstructed_img = torch.mm(W, H)\n",
    "\n",
    "    # convert the reconstructed 2D tensor to 3D tensor\n",
    "    reconstructed_img3D = reconstructed_img.reshape(3, crop.shape[1], -1)\n",
    "\n",
    "    # Reshaping and Scaling \n",
    "    reconstructed_img_scaled = reshaping(reconstructed_img3D)\n",
    "    \n",
    "    # calculate metrics\n",
    "    RMSE, PSNR = metrics(reconstructed_img3D, crop)\n",
    "\n",
    "    #plot reconstructed and masked image\n",
    "    masked_img = masked_img.to(img.dtype)\n",
    "    plot_reconstructed_img(reconstructed_img_scaled, masked_img, RMSE, PSNR, title, N_size)\n",
    "\n",
    "    return masked_img, RMSE, PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a rectangular block of 30X30 is assumed missing from the image.\n",
    "masked_img_a, RMSE_a, PSNR_b = reconstruct_img(img, 100, 30, 700, 875, 'A rectangular block of 30X30 is assumed missing', rect_patch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a random subset of 900 (30X30) pixels is missing from the image.\n",
    "masked_img_b, RMSE_b, PSNR_b = reconstruct_img(img, 100, 30, 700, 875, 'A random subset of 900 (30X30) pixels is missing', rect_patch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.io.read_image(\"dog.jpg\")\n",
    "img = img.float() / 255.0 \n",
    "crop = torchvision.transforms.functional.crop(img, 500, 675, 400, 400)\n",
    "plt.imshow(rearrange(crop, 'c h w -> h w c').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RFF features\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_rff_features(X, num_features, sigma):\n",
    "    from sklearn.kernel_approximation import RBFSampler\n",
    "    rff = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2),random_state=42)\n",
    "    X = X.cpu().numpy()\n",
    "    X = rff.fit_transform(X)\n",
    "    return torch.tensor(X, dtype=torch.float32).to(device)\n",
    "\n",
    "def create_coordinate_map(img, scale=1):\n",
    "    \"\"\"\n",
    "    img: torch.Tensor of shape (num_channels, height, width)\n",
    "    \n",
    "    return: tuple of torch.Tensor of shape (height * width, 2) and torch.Tensor of shape (height * width, num_channels)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_channels, height, width = img.shape\n",
    "    \n",
    "    # Create a 2D grid of (x,y) coordinates (h, w)\n",
    "    # width values change faster than height values\n",
    "    w_coords = torch.arange(0, width,  1/scale).repeat(int(height*scale), 1)\n",
    "    h_coords = torch.arange(0, height, 1/scale).repeat(int(width*scale), 1).t()\n",
    "    w_coords = w_coords.reshape(-1)\n",
    "    h_coords = h_coords.reshape(-1)\n",
    "\n",
    "    # Combine the x and y coordinates into a single tensor\n",
    "    X = torch.stack([h_coords, w_coords], dim=1).float()\n",
    "\n",
    "    # Move X to GPU if available\n",
    "    X = X.to(device)\n",
    "\n",
    "    # Reshape the image to (h * w, num_channels)\n",
    "    Y = rearrange(img, 'c h w -> (h w) c').float()\n",
    "    return X, Y\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "def train(net, lr, X, Y, epochs, verbose=True):\n",
    "    \"\"\"\n",
    "    net: torch.nn.Module\n",
    "    lr: float\n",
    "    X: torch.Tensor of shape (num_samples, 2)\n",
    "    Y: torch.Tensor of shape (num_samples, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X)\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if verbose and epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch} loss: {loss.item():.6f}\")\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_predict_with_rff_tensor(image_tensor,device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Train a model with RFF on the image tensor with missing data and predict the missing portions.\n",
    "    \"\"\"\n",
    "    X, Y = create_coordinate_map(image_tensor)\n",
    "    nan_mask_Y = torch.isnan(Y).any(dim=1)\n",
    "    X_filtered = X[~nan_mask_Y]\n",
    "    Y_filtered = Y[~nan_mask_Y]\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_X = scaler.fit_transform(X_filtered)\n",
    "    scaled_X_filtered = torch.tensor(scaled_X, device=device)\n",
    "\n",
    "    X_rff = create_rff_features(scaled_X_filtered, 1000, 0.08)\n",
    "    \n",
    "    net = LinearModel(X_rff.shape[1], 3)\n",
    "    net.to(device)\n",
    "    train(net, 0.005, X_rff, Y_filtered, 1000, verbose=False)\n",
    "    \n",
    "    scaled_X = scaler.transform(X)\n",
    "    scaled_X_tensor = torch.tensor(scaled_X, device=device)\n",
    "    X_rff_new = create_rff_features(scaled_X_tensor, 1000, 0.08)\n",
    "    \n",
    "    outputs = net(X_rff_new)\n",
    "    outputs_np = outputs.detach().numpy()\n",
    "    \n",
    "    predicted_image_tensor = outputs_np\n",
    "\n",
    "    return predicted_image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructed_img_rff(reconstructed_img, masked_img,title):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(5, 2.5))  \n",
    "    predicted_image_np = np.reshape(reconstructed_img,(masked_img.shape[1],masked_img.shape[2],masked_img.shape[0]))\n",
    "    axs[0].imshow(predicted_image_np)\n",
    "    axs[0].set_title('Reconstructed Image using RFF')        \n",
    "    axs[1].imshow(rearrange(masked_img, 'c h w -> h w c'))\n",
    "    axs[1].set_title(\"Original Image\")\n",
    "    plt.suptitle(title, weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_img = mask_image_structured(crop, (200, 200), 30)\n",
    "reconstructed_img= train_and_predict_with_rff_tensor(masked_img,device=device)\n",
    "plot_reconstructed_img_rff(reconstructed_img, masked_img, \"A Rectangular Block of 30X30 is Assumed Missing\")\n",
    "predicted_image_np = torch.tensor(np.reshape(reconstructed_img,(masked_img.shape[0], masked_img.shape[1],masked_img.shape[2])))\n",
    "\n",
    "RMSE_1, PSNR_1 = metrics(predicted_image_np ,crop)\n",
    "print(\"Reconstruction using RFF+Linear Regression\")\n",
    "print(\"RMSE for structured missing regions: \", RMSE_1)\n",
    "print(\"PSNR for structured missing regions: \", PSNR_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_img = mask_image_unstructured(crop, 30)\n",
    "reconstructed_img= train_and_predict_with_rff_tensor(masked_img,device=device)\n",
    "plot_reconstructed_img_rff(reconstructed_img, masked_img, 'A Random Subset of 900 (30X30) Pixels is Missing')\n",
    "predicted_image_np = torch.tensor(np.reshape(reconstructed_img,(masked_img.shape[0], masked_img.shape[1],masked_img.shape[2])))\n",
    "\n",
    "RMSE_2, PSNR_2 = metrics(predicted_image_np ,crop)\n",
    "print(\"Reconstruction using RFF+Linear Regression\")\n",
    "print(\"RMSE for unstructured missing regions: \", RMSE_2)\n",
    "print(\"PSNR for unstructured missing regions: \", PSNR_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.io.read_image(\"dog.jpg\")\n",
    "print(img.shape)\n",
    "plt.imshow(rearrange(img, 'c h w -> h w c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(N_values, RMSE_values, PSNR_values):\n",
    "    print(\"N_values:\", N_values)\n",
    "    print(\"RMSE_values:\", RMSE_values)\n",
    "    print(\"PSNR_values:\", PSNR_values)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(6, 3))  \n",
    "    axs[0].plot(N_values, RMSE_values, marker='o', label='RMSE')\n",
    "    axs[0].set_title('RMSE vs N size')\n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].plot(N_values, PSNR_values, marker='o', label='PSNR')\n",
    "    axs[1].set_title('PSNR vs N size')\n",
    "    axs[1].grid(True)\n",
    "    axs[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a rectangular block of 30X30 is assumed missing from the image.\n",
    "N = [20, 40, 60 ,80]\n",
    "RMSE_a = []\n",
    "PSNR_a = []\n",
    "for i in range(4):\n",
    "    masked_img_a, RMSE, PSNR = reconstruct_img(img, 100, N[i], 700, 875, f'A Rectangular subset of {N[i] * N[i]} ({N[i]}X{N[i]}) pixels is missing', rect_patch=1)\n",
    "    RMSE_a.append(RMSE)\n",
    "    PSNR_a.append(PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(N, RMSE_a, PSNR_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_b = []\n",
    "PSNR_b = []\n",
    "for i in range(4):\n",
    "    masked_img_b, RMSE, PSNR = reconstruct_img(img, 100, N[i], 700, 875, 'a Rectangular block of 30X30 is assumed missing from the image', rect_patch=0)\n",
    "    RMSE_b.append(RMSE)\n",
    "    PSNR_b.append(PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(N, RMSE_b, PSNR_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
